{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g84daf-5RgOq"
      },
      "source": [
        "# All Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N3IGpJhRgOr",
        "outputId": "abd82d43-3ed4-49a5-c4cb-ad05054be053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy: 1.23.1 Scipy: 1.9.1 Pandas 1.5.0\n",
            "Pytorch Normal: 1.12.1\n",
            "Pytorch Lightning: 1.7.6\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "print (\"Numpy:\",np.__version__ , \"Scipy:\",sp.__version__, \"Pandas\",pd.__version__)\n",
        "\n",
        "#pytorch and pytorch lightning API\n",
        "import torch\n",
        "# import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.cuda as cuda\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from datetime import datetime\n",
        "print (\"Pytorch Normal:\", torch.__version__)\n",
        "print (\"Pytorch Lightning:\", pl.__version__)\n",
        "\n",
        "#weights and biases for monitoring DL training progress \n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "#plotting\n",
        "from IPython import display\n",
        "# from matplotlib import pyplot as plt\n",
        "# import matplotlib.cm as cm\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# import plotly.express as px\n",
        "# import plotly.figure_factory as ff\n",
        "# import plotly.graph_objs as go\n",
        "\n",
        "#sklearn\n",
        "# from sklearn.decomposition import PCA \n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import cv2\n",
        "# import PIL\n",
        "# import imageio\n",
        "# import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpTbtVSfRgOs",
        "outputId": "70ab9c06-66b9-4e74-e4ec-aa582fab7948"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, 0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cuda.is_available(),cuda.device_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6xmSTfgRgOt"
      },
      "source": [
        "# SHARE data  extraction Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Wave 8 Release 8.0.0 from  https://releases.sharedataportal.eu/releases . You would require to apply for data access if you already don't have it."
      ],
      "metadata": {
        "id": "Q3AVgisoTFMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBS6GZzKRgOt"
      },
      "outputs": [],
      "source": [
        "df_ac = pd.read_stata(\"my/path/sharew8_rel8-0-0_ALL_datasets_stata/sharew8_rel8-0-0_ac.dta\")\n",
        "df_gv = pd.read_stata(\"my/path/sharew8_rel8-0-0_ALL_datasets_stata/sharew8_rel8-0-0_gv_health.dta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzKsaC68RgOt"
      },
      "outputs": [],
      "source": [
        "df_ac = df_ac.set_index(\"mergeid\")\n",
        "df_gv= df_gv.set_index(\"mergeid\") \n",
        "df_gv_rearr = df_gv.loc[df_ac.index]\n",
        "(df_ac.index == df_gv_rearr.index).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2hNnoQPRgOt"
      },
      "outputs": [],
      "source": [
        "np.min(df_gv_rearr[\"casp\"]), np.max(df_gv_rearr[\"casp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T5UHGJ7RgOt"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(df_ac, df_gv_rearr, left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9FImd57RgOt"
      },
      "outputs": [],
      "source": [
        "df2 = df[[\"ac014_\",\"ac015_\",\"ac016_\",\"ac017_\",\"ac018_\",\"ac019_\",\"ac020_\",\"ac021_\",\"ac022_\",\"ac023_\",\"ac024_\",\"ac025_\", \"casp\"]]\n",
        "df3 = df2.replace({\"Refusal\":np.NAN, \"Don't know\": np.nan, \"Often\": 3, \"Sometimes\": 2, \"Rarely\": 1, \"Never\": 0})\n",
        "# df3.isna().sum()\n",
        "df4 = df3.dropna()\n",
        "df3.shape , df4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2FZmAcdRgOu"
      },
      "outputs": [],
      "source": [
        "x_data = df4.iloc[:,0:12].to_numpy()\n",
        "y_data = df4.iloc[:,12:13].to_numpy()/50 # we are linearly scaling by deviding it with 50 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_U-eW_QRgOu"
      },
      "outputs": [],
      "source": [
        "x_train,x_test, y_train,y_test  = train_test_split(x_data, y_data, test_size=0.2, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC9i9JnPRgOu"
      },
      "outputs": [],
      "source": [
        "np.save(\"my/path/x_train.npy\", x_train)\n",
        "np.save(\"my/path/y_train.npy\", y_train)\n",
        "np.save(\"my/path/x_test.npy\", x_test)\n",
        "np.save(\"my/path/y_test.npy\", y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vYTPwiXRgOu"
      },
      "outputs": [],
      "source": [
        "# x_train = np.load(\"my/path/x_train.npy\", allow_pickle= True)\n",
        "# y_train = np.load(\"my/path/y_train.npy\",  allow_pickle= True)\n",
        "\n",
        "# x_test = np.load(\"my/path/x_test.npy\",  allow_pickle= True)\n",
        "# y_test = np.load(\"my/path/y_test.npy\",  allow_pickle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mZdBZgdRgOu"
      },
      "outputs": [],
      "source": [
        "# x_train_c1,x_train_c2,y_train_c1,y_train_c2 = train_test_split(x_train,y_train, test_size=0.5, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKyN9AzKRgOv"
      },
      "outputs": [],
      "source": [
        "# n_components = 3\n",
        "# pca = PCA(n_components=n_components)\n",
        "# components = pca.fit_transform(x)\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     components,\n",
        "#     color=y.reshape(y.shape[0]),\n",
        "#     # labels=labels,\n",
        "#     # symbol=symbol,\n",
        "#     dimensions=range(n_components),\n",
        "#     # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV8pqKcQRgOv"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iAy6zRaRgOv"
      },
      "source": [
        "## Import data extracted from SHARE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg7oHRqrRgOv",
        "outputId": "bb77e96b-ce26-4488-b53e-d4918857b0d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((34315, 12), (34315, 1))"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x= np.load(\"my/path/x_train.npy\", allow_pickle=True)\n",
        "y = np.load(\"my/path/y_train.npy\",allow_pickle=True)\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXxKtw-WRgOv",
        "outputId": "b2b46338-d35f-4998-d4c3-3dea284067ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8579, 12), (8579, 1))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test= np.load(\"my/path/x_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"my/path/y_test.npy\",allow_pickle=True)\n",
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTJFdQudRgOv"
      },
      "outputs": [],
      "source": [
        "#split again to get the train and validation\n",
        "x_train_bar,x_val,y_train_bar,y_val= train_test_split(x,y, test_size=0.1)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fcOCXLSRgOv"
      },
      "outputs": [],
      "source": [
        "#apply one hot encoding\n",
        "x_train_bar_cat = F.one_hot(torch.tensor(x_train_bar.astype(\"int\")), num_classes=4)\n",
        "x_val_cat = F.one_hot(torch.tensor(x_val.astype(\"int\")), num_classes=4)\n",
        "x_test_cat = F.one_hot(torch.tensor(x_test.astype(\"int\")), num_classes=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up1GZKRuRgOv"
      },
      "source": [
        "### create data for federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwA38xjzRgOv"
      },
      "outputs": [],
      "source": [
        "x= np.load(\"my/path/x_train.npy\", allow_pickle=True)\n",
        "y = np.load(\"my/path/y_train.npy\",allow_pickle=True)\n",
        "x_test= np.load(\"my/path/x_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"my/path/y_test.npy\",allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXZthTcnRgOv"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame( np.hstack ((x,y)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN0Gi9eJRgOv"
      },
      "outputs": [],
      "source": [
        "def split_by_fractions(df: pd.DataFrame, fracs: list, random_state: int):\n",
        "    df = df.sample(frac=1.0, random_state=random_state)\n",
        "    assert sum(fracs) == 1.0, 'fractions sum is not 1.0 (fractions_sum={})'.format(sum(fracs))\n",
        "    remain = df.index.copy().to_frame()\n",
        "    res = []\n",
        "    for i in range(len(fracs)):\n",
        "        fractions_sum = sum(fracs[i:])\n",
        "        frac = fracs[i] / fractions_sum\n",
        "        idxs = remain.sample(frac=frac, random_state=random_state).index\n",
        "        remain = remain.drop(idxs)\n",
        "        res.append(idxs)\n",
        "    return [df.loc[idxs].reset_index(drop=True) for idxs in res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8RJccxHRgOw"
      },
      "outputs": [],
      "source": [
        "p1,p2,p3,p4,p5 = split_by_fractions (data, fracs=[0.1, 0.15, 0.15, 0.3, 0.3], random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_sCLvLqRgOw"
      },
      "outputs": [],
      "source": [
        "xp1 = p1.iloc[:,0:12].to_numpy()\n",
        "yp1 = p1.iloc[:,12:13].to_numpy()\n",
        "xp2 = p2.iloc[:,0:12].to_numpy()\n",
        "yp2 = p2.iloc[:,12:13].to_numpy()\n",
        "xp3 = p3.iloc[:,0:12].to_numpy()\n",
        "yp3 = p3.iloc[:,12:13].to_numpy()\n",
        "xp4 = p4.iloc[:,0:12].to_numpy()\n",
        "yp4 = p4.iloc[:,12:13].to_numpy()\n",
        "xp5 = p5.iloc[:,0:12].to_numpy()\n",
        "yp5 = p5.iloc[:,12:13].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoyPCEGSRgOw"
      },
      "outputs": [],
      "source": [
        "xp1_cat= F.one_hot(torch.tensor(xp1.astype(\"int\")), num_classes=4)\n",
        "xp2_cat= F.one_hot(torch.tensor(xp2.astype(\"int\")), num_classes=4)\n",
        "xp3_cat= F.one_hot(torch.tensor(xp3.astype(\"int\")), num_classes=4)\n",
        "xp4_cat= F.one_hot(torch.tensor(xp4.astype(\"int\")), num_classes=4)\n",
        "xp5_cat= F.one_hot(torch.tensor(xp5.astype(\"int\")), num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBu_GuFYRgOw"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.save(\"my/path/xp1_cat.npy\", xp1_cat)\n",
        "np.save(\"my/path/xp2_cat.npy\", xp2_cat)\n",
        "np.save(\"my/path/xp3_cat.npy\", xp3_cat)\n",
        "np.save(\"my/path/xp4_cat.npy\", xp4_cat)\n",
        "np.save(\"my/path/xp5_cat.npy\", xp5_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtBp8I5sRgOw"
      },
      "source": [
        "### Load data for individual central mode training "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is only for training on individual client or participant; \n",
        "\n",
        "Do not run for centralmode, for central model directly go to hyperparameters "
      ],
      "metadata": {
        "id": "WTF7ixUQYKZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFvVvR7-RgOw"
      },
      "outputs": [],
      "source": [
        "\n",
        "xp1_cat = np.load(\"my/path/xp1_cat.npy\", allow_pickle= True)\n",
        "xp2_cat = np.load(\"my/path/xp2_cat.npy\", allow_pickle= True)\n",
        "xp3_cat = np.load(\"my/path/xp3_cat.npy\", allow_pickle= True)\n",
        "xp4_cat = np.load(\"my/path/xp4_cat.npy\",allow_pickle= True)\n",
        "xp5_cat = np.load(\"my/path/xp5_cat.npy\",allow_pickle= True)\n",
        "\n",
        "yp1 = np.load(\"my/path/yp1.npy\", allow_pickle= True)\n",
        "yp2 = np.load(\"my/path/yp2.npy\",allow_pickle= True)\n",
        "yp3 = np.load(\"my/path/yp3.npy\",allow_pickle= True)\n",
        "yp4 = np.load(\"my/path/yp4.npy\",allow_pickle= True)\n",
        "yp5 = np.load(\"my/path/yp5.npy\",allow_pickle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a46t-ugWRgOw"
      },
      "outputs": [],
      "source": [
        "# Only change this part and run the rest\n",
        "x = xp5_cat\n",
        "y = np.array(yp5, dtype= 'float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFzbeRnXRgOw"
      },
      "outputs": [],
      "source": [
        "x_train_bar_cat,x_val_cat,y_train_bar,y_val= train_test_split(x,y, test_size=0.1)\n",
        "# remember these are already cartegorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDOiCx5xRgOw"
      },
      "outputs": [],
      "source": [
        "x_test_cat = F.one_hot(torch.tensor(x_test.astype(\"int\")), num_classes=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B3lZQYmRgOw"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A5tY68cRgOw"
      },
      "outputs": [],
      "source": [
        "input_size = (12,4)\n",
        "num_epochs = 300\n",
        "# every_n_train_steps=25\n",
        "batch_size = 128\n",
        "learning_rate = 1e-4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opKUcomzRgOw"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNivxvqIRgOw",
        "outputId": "c2c22506-8772-4133-a9b1-00ce864aee1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/lj/w83k8rr9017bd3y925h27x4c0000gp/T/ipykernel_67540/2828369402.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_test_tt = torch.tensor(x_test_cat, dtype=torch.float)#.unsqueeze(1)\n"
          ]
        }
      ],
      "source": [
        "x_train_tt = torch.tensor(x_train_bar_cat,dtype=torch.float)\n",
        "x_val_tt = torch.tensor(x_val_cat, dtype=torch.float)\n",
        "x_test_tt = torch.tensor(x_test_cat, dtype=torch.float)\n",
        "y_train_tt = torch.tensor(y_train_bar,dtype=torch.float)\n",
        "y_val_tt = torch.tensor(y_val, dtype=torch.float)\n",
        "y_test_tt = torch.tensor(y_test, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbqAT3M6RgOx"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train_tt, y_train_tt)\n",
        "val_dataset = TensorDataset(x_val_tt, y_val_tt)\n",
        "test_dataset = TensorDataset(x_test_tt, y_test_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NSIMyu4RgOx"
      },
      "outputs": [],
      "source": [
        "# change num workers as per number of cores in cpu available for computation \n",
        "training_loader = DataLoader(dataset= train_dataset, batch_size=batch_size, shuffle= True, num_workers=8)\n",
        "validation_loader = DataLoader(dataset= val_dataset, batch_size=batch_size, shuffle= False, num_workers=8)\n",
        "test_loader = DataLoader (dataset= test_dataset, batch_size=batch_size, shuffle= False, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-QYzHTPRgOx"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zkPalmURgOx"
      },
      "outputs": [],
      "source": [
        "class Pred(pl.LightningModule):\n",
        "    def __init__(self, input_size,learning_rate ):\n",
        "        super(Pred, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fc1 = nn.Linear(48, 16)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=16, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self,x): \n",
        "        x= torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x= self.drop(x)\n",
        "        x= self.bn1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.tanh(x)\n",
        "        return (x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, label = batch\n",
        "        y_hat = self(data)\n",
        "        loss = F.mse_loss(y_hat, label) \n",
        "        self.log(\"train/loss\", loss, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data,label = batch\n",
        "        y_hat = self(data)\n",
        "        loss = F.mse_loss(y_hat, label) \n",
        "        self.log(\"valid/val_loss\", loss, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        return optimizer    \n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        checkpoint[\"hyperparameters\"] = (input_size, num_epochs, batch_size, learning_rate)\n",
        "    \n",
        "    def on_load_checkpoint(self, checkpoint):\n",
        "        input_size, num_epochs, batch_size, learning_rate = checkpoint[\"hyperparameters\"] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReMA3s_HRgOx"
      },
      "outputs": [],
      "source": [
        "model = Pred(input_size=input_size, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plr_yCChRgOx"
      },
      "outputs": [],
      "source": [
        "#log in to weights and biases if you want to monitor the model, else leave this cell\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgXPpv97RgOx"
      },
      "outputs": [],
      "source": [
        "! mkdir my/path/m0t0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training "
      ],
      "metadata": {
        "id": "e1yVmIhocGoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the following code on availibility of your GPU, in our case we trained on CPU\n",
        "\n",
        "documentation of lightning trainer : https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
        "\n",
        "change the logger arguement in trainer if you are not using weights and biases. "
      ],
      "metadata": {
        "id": "af2aQMM7aO9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwAPK6TlRgOx"
      },
      "outputs": [],
      "source": [
        "wandb_logger = WandbLogger(name='m0t0',project='DL') \n",
        "wandb_logger.watch(model, log=\"all\", log_graph=True) # log frequency != num epochs to print loos rather num steps \n",
        "checkpoint_callback = ModelCheckpoint(dirpath='my/path/m0t0',\n",
        "                                      filename='{epoch}-{training_loss:.2f}-{val_loss:.2f}',\n",
        "                                      # monitor=\"valid/val_loss\", mode = \"min\",\n",
        "                                      # every_n_train_steps=every_n_train_steps,\n",
        "                                      # every_n_epochs= 25\n",
        "                                      verbose=True,\n",
        "                                      # save_on_train_epoch_end=True,\n",
        "                                      save_last = True,\n",
        "                                      every_n_epochs= 25,\n",
        "                                      save_top_k=-1\n",
        "                                      )\n",
        "trainer = pl.Trainer(max_epochs=num_epochs,\n",
        "                     callbacks=[ checkpoint_callback],\n",
        "                    #  accelerator=\"gpu\", amp_backend=\"native\")\n",
        "                     logger=wandb_logger)\n",
        "                    #  callbacks=[EarlyStopping(monitor=\"val/val_loss\", mode=\"min\")])\n",
        "# trainer = pl.Trainer\n",
        "trainer.fit(model=model, train_dataloaders=training_loader, val_dataloaders= validation_loader)\n",
        "\n",
        "# trainer.test(dataloaders=test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "A3BjdnANcOO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to load aved model from other epochs use following cell else not. "
      ],
      "metadata": {
        "id": "7WZQvk4SasS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob9JuGgWRgOx"
      },
      "outputs": [],
      "source": [
        "checkpoint= \"my/path/m0t0/epoch=299-training_loss=0.00-val_loss=0.00.ckpt\"\n",
        "model = Pred(input_size=input_size, learning_rate=learning_rate)\n",
        "model_trained = model.load_from_checkpoint(checkpoint, input_size=input_size, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate accuracy metrics in whole 20 percent test dataset that we kept separate."
      ],
      "metadata": {
        "id": "GfaJHCbGa_F_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur6zh_hdRgOx"
      },
      "outputs": [],
      "source": [
        "# pred = model(x_test_tt).detach().numpy()\n",
        "pred = model_trained(x_test_tt).detach().numpy()\n",
        "# pred.shape, y_test.shape\n",
        "#RMSE\n",
        "print (\"RMSE:\", (np.sum(np.square(pred-y_test))/y_test.shape[0])**0.5)\n",
        "print(\"MAE:\", np.sum(np.abs(pred-y_test))/y_test.shape[0])\n",
        "\n",
        "print(\"R_square:\", 1 - (np.sum(np.square(pred-y_test))) / np.sum(np.square(y_test-np.average(y_test))) )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are training individual model then use this to calculate accuracy metrics tested on own test set. This is not required for Central model."
      ],
      "metadata": {
        "id": "keHqGRuTbM_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD36ymafRgOy"
      },
      "outputs": [],
      "source": [
        "# pred = model(x_val_tt).detach().numpy()\n",
        "pred = model_trained(x_val_tt).detach().numpy()\n",
        "# pred.shape, y_val.shape\n",
        "#RMSE\n",
        "print (\"RMSE:\", (np.sum(np.square(pred-y_val))/y_val.shape[0])**0.5)\n",
        "print(\"MAE:\", np.sum(np.abs(pred-y_val))/y_val.shape[0])\n",
        "\n",
        "print(\"R_square:\", 1 - (np.sum(np.square(pred-y_val))) / np.sum(np.square(y_val-np.average(y_val))) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0QbZ6jXRgOy"
      },
      "outputs": [],
      "source": [
        "# Finish weights and bias task. \n",
        "\n",
        "wandb.finish"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('dl1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cd8d9cb0b55535ff2b667e3598ed291f59ea9d2ceaed29a9ff3adf33a58b7093"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g84daf-5RgOq",
        "e6xmSTfgRgOt"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}